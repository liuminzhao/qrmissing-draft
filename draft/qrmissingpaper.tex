\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage[round]{natbib}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{mathpazo}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsthm}
\geometry{verbose,letterpaper,tmargin=1in,bmargin=.75in,lmargin=.75in,rmargin=1in}
\usepackage{booktabs}

\title{Quantile Regression in the Presence of Monotone Missingness}
\date{\today}
\author{}

\newtheorem{thm}{Theorem}[section]
\newtheorem{deff}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
% \newtheorem{prf}[thm]{Proof}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{emp}[thm]{Example}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pps}[thm]{Proposition}

\newcommand{\iid}{\stackrel{\text{i.i.d}}{\sim}}
\DeclareMathOperator{\pr}{p}
\DeclareMathOperator{\prob}{Pr}
\newcommand{\polya}{P\'{o}lya}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Quantile regression is used to study the relationship between a
response and covariates when one (or several) quantiles are of
interest as opposed to mean regression.  The dependence between upper
or lower quantiles of the response variable and the covariates often
vary differentially relative to that of the mean. How quantiles depend
on covariates is of interest in econometrics, educational studies,
biomedical studies, and environment studies \citep{yu2001,
  buchinsky1994,buchinsky1998,he1998, koenker1999,wei2006,yu2003}. A
comprehensive review of applications of quantile regression was
presented in \citet{koenker2005}.

Quantile regression is more robust to outliers than mean regression
and provides information about how covariates affect quantiles, which
offers a more complete description of the conditional distribution of
the response. Different effects of covariates can be assumed for
different quantiles.

The traditional frequentist approach was proposed by
\citet{koenker1978} for a single quantile with estimators derived by
minimizing a loss function. The popularity of this approach is due to
its computational efficiency, well-developed asymptotic properties,
and straightforward extensions to simultaneous quantile regression and
random effect models. However, asymptotic inference may not be
accurate for small sample sizes and not naturally extend to missing
data.

Bayesian approaches offer exact inference in small samples. Motivated
by the loss (check) function, \citet{yu2001} proposed an asymmetric
Laplace distribution for the error term, such that maximizing the
posterior distribution is equivalent to minimizing the check function.
Semiparametric methods have been proposed for median
regression. \citet{walker1999} used a diffuse finite \polya{} Tree
prior for the error term. \citet{kottas2001} modeled the error by two
families of median zero distribution using a mixture Dirichlet process
priors, which is very useful for unimodal error
distributions. \citet{hanson2002} adopted mixture of \polya{} Tree
prior in median regression, which is more robust in terms of
multimodality and skewness. Other recent approaches include quantile
pyramid priors, mixture of Dirichlet process priors of multivariate
normal distributions and infinite mixture of Gaussian densities which
put quantile constraints on the residuals \citep{hjort2007, hjort2009,
  kottas2009,reich2010}.

The above methods focus on complete data without missingness.  There
are a few more articles about quantile regression with missingness.
\citet{yuan2010} introduced a Bayesian quantile regression approach
for longitudinal data with nonignorable missing data. They used random
effects to explain the within-subject correlation and applied a $l_2$
penalty in the traditional quantile regression check function to
shrink toward the common population values. However, the quantile
regression coefficients are conditional on the random effects, which
is not of interest if we are looking into the marginal relationship.
\citet{wei2012} proposed a multiple imputation method for quantile
regression model when there are some covariates missing at
random. They impute the missing covariates by specifying the its
conditional density given observed covariates and outcomes, which
comes from the estimated conditional quantile regression and
specification of conditional density of missing covariates given
observed ones.  However, they put more focus on the missing covariates
rather than missing outcomes.  \citet{bottai2013} illustrated a new
imputation method by estimated conditional quantiles of missing
outcomes given observed data. Their approach does not make
distribution assumptions. Their method also has advantages as
robustness to outliers and invariance to transformations.
\citet{roy2008} proposed a pattern mixture model for data with
nonignorable dropout, which borrowed idea from \citet{heagerty1999}.
But their methods examine the marginal covariate effects on the
mean. We will use these ideas for quantile regression models.

The structure of this article is as follows. First, we introduce a
quantile regression method to address monotone nonignorable dropout
missingness in section \ref{sec:model}, including sensitivity analysis
and computational details.  We use simulation studies to evaluate the
performance of the model in section \ref{sec:simulation}. We apply our
approach to data from a recent clinical trial in section
\ref{sec:real}. Finally, discussion and conclusions are in section
\ref{sec:discussion}.

\section{Model}
\label{sec:model}

In this section, we first introduce some notation
, then describe our proposed quantile regression model in
section \ref{sec:settings}. We provide details on MAR and MNAR
and computation in sections \ref{sec:sa} and \ref{sec:computation}.

Under monotone dropout, without loss of generality, denote $S_i \in
\{1, 2, \ldots, J\}$ to be the follow up time, and $\bm Y_i = (Y_{i1},
Y_{i2}, \ldots, Y_{iJ})^{T}$ to be the response vector for subject
$i$, where $J$ is the maximum follow up time. We assume $Y_{i1}$ is
always observed. We are interested in the $\tau$-th marginal quantile
regression coefficients $\bm \gamma_j = (\gamma_{j0}, \gamma_{j2},
\ldots, \gamma_{jp})^T$,
\begin{equation}
  \label{eq:quantile}
  \prob (Y_{ij} \leq \bm x_i^{T} \bm \gamma_j ) = \tau, \text{ for } j = 1, \ldots, J,
\end{equation}
where $\bm x_i$ is a $p \times 1$ vector of covariates for subject $i$.

Let
\begin{align*}
  \pr_k(Y) &= \pr (Y | S = k), \\
  \pr_{\geq k} (Y) & = \pr (Y | S \geq k)
\end{align*}
be the densities of response $\bm Y$ given follow-up time $S=k$ and $S
\geq k$. And $\prob_k$ be the corresponding probability given $S = k$.

\subsection{Mixture Model Specification}
\label{sec:settings}
We adopt a pattern mixture model to jointly model the response and
missingness.  Let the subscript $i$ stand for subject $i$. Specify the
conditional distribution as:
\begin{equation}
  \label{eq:model}
  \left.  \begin{aligned}
      & \pr_k(y_{i1}) = \textrm{N} (\Delta_{i1} + \bm x_{i1}^T \bm \beta_1^{(k)},
      \exp (\bm x_{i1}^T \bm \alpha_1^{(k)} ) ), k = 1, \ldots, J,\\
      &\pr_k(y_{ij}|\bm y_{i1:i(j-1)}) =
      \begin{cases}
        \textrm{N} \big (\Delta_{ij} + \bm x_{ij}^T \bm h_{j}^{(k)} +
        \bm y_{i1:i(j-1)}^T \bm \beta_{y,j-1}^{(k)},
        \exp (\bm x_{ij}^T \bm \alpha_j^{(k)} ) \big), & k < j ;  \\
        \textrm{N} \big (\Delta_{ij} + \bm y_{i1:i(j-1)}^T \bm
        \beta_{y,j-1}^{(\geq j)},
        \exp (\bm x_{ij}^T \bm \alpha_j^{(\geq j)} ) \big), & k \geq j ;  \\
      \end{cases}, \text{ for } 2 \leq j \leq J,  \\
      &S_{ij} = k| \bm x_{ij} \sim \textrm{Multinomial}(1, \bm \pi),
    \end{aligned} \right\}
\end{equation}
where $\bm y_{i1:i(j-1)} = (y_{i1}, \ldots, y_{i(j-1)})^T, \bm \pi =
(\pi_1, \ldots, \pi_J)$,$\bm h_j^{(k)} = (h_{j1}^{(k)}, \ldots,
h_{jp}^{(k)})$, $\bm x_j$ is a $p \times 1$ covariate vector, $\bm
\beta_{y, j-1}^{(k)} = \big(\beta_{y_1, j-1}^{(k)}, \ldots,
\beta_{y_{j-1}, j-1}^{(k)} \big)^T$ and $\bm \alpha_j^{(k)}$ is a $p
\times 1$ vector controlling heterogeneity of conditional variance of
response component $j$.

In (\ref{eq:model}), $\Delta_{ij}$ are functions of $\tau, \bm
x_{ij}, \bm \alpha_j$ and other parameters and are determined by the
marginal quantile regressions,
\begin{align}
  \label{eq:deltaeqn1}
  \tau = \prob (Y_{ij} \leq \bm x_{ij}^T \bm \gamma_j ) = \sum_{k=1}^J
  \pi_k\prob_k (Y_{ij} \leq \bm x_{ij}^T \bm \gamma_j ),
\end{align}
for $j = 1$ and
\begin{align}\label{eq:deltaeqn2}
  \tau &= \prob (Y_{ij} \leq \bm x_{ij}^{T} \bm \gamma_j ) =
  \sum_{k=1}^J
  \pi_k\prob_k (Y_{ij} \leq \bm x_{ij}^{T} \bm \gamma_j ) \\
  & = \sum_{k=1}^J \pi_k \int\cdots \int \prob_k (Y_{ij} \leq \bm
  x_{ij}^{T} \bm \gamma_j |y_{i1},\ldots,
  y_{i(j-1)}) \pr_k (y_{i(j-1)}| y_{i1}, \ldots, y_{i(j-2)})  \nonumber \\
  & \quad \cdots \pr_k (y_{i2}| y_{i1}) \pr_k(y_{i1})
  dy_{i(j-1)}\cdots dy_{i1}. \nonumber
\end{align}
for $j = 2, \ldots, J$. More computational details will be given in
section \ref{sec:computation}.

The idea is to model the marginal quantile regressions directly, then
to embed them in the likelihood through restrictions in the mixture
model. The mixture model and heterogeneity of variance between
subjects allows the marginal quantile regression coefficients to
differ by quantiles. Otherwise, the quantile lines would be parallel
to each other.

For identifiability, we apply the following restrictions,
\begin{align*}
  & \sum_{k=1}^J \beta_{l1}^{(k)} = 0, l = 1,\ldots, p,
\end{align*}
where $\bm \beta_1^{(k)} = (\beta_{11}^{(k)}, \ldots,
\beta_{p1}^{(k)})^{T}$. Further details on these restrictions can be
found in appendix \ref{sec:iden}.

\subsection{Missing Data Mechanism and Sensitivity Analysis}
\label{sec:sa}

In the mixture model in (\ref{eq:model}), MAR holds \citep{molen1998}
if and only if, for each $j \geq 2$ and $k < j$:
\begin{equation}
  \label{eq:molen}
  \pr_k(y_j|y_1, \ldots, y_{j-1}) = \pr_{\geq j}(y_j|y_1, \ldots, y_{j-1}).
\end{equation}
When $2 \leq j \leq J$ and $k < j$, $Y_j$ is not observed, thus $\bm
h_j^{(k)}$ and $\bm \alpha_j^{(k)}$, $ \bm \beta_{y, j-1}^{(k)} =
\big(\beta_{y_1,j}^{(k)}, \ldots, \beta_{y_{j-1},j-1}^{(k)} \big)^T $
can not be identified from the observed data. Denote
\begin{align*}
  \bm \alpha_j^{(k)} &= \bm \alpha_j^{(\geq j)} + \bm \delta_{j}^{(k)}, \\
  \bm \beta_{y, j-1}^{(k)} &= \bm \beta_{y, j-1}^{(\geq j)} + \bm
  \eta_{j-1}^{(k)},
\end{align*}
where $\bm \delta_j^{(k)} = \big( \delta_{1j}^{(k)}, \ldots,
\delta_{pj}^{(k)} \big)$ and $\bm \eta_{j-1}^{(k)} = \big(
\eta_{y_1,j-1}^{(k)}, \ldots, \eta_{y_{j-1}, j-1}^{(k)} \big)$ for $k
< j$. Then $\bm \xi_s = ( \bm h_j^{(k)}, \bm \eta_{j-1}^{(k)}, \bm
\delta_j^{(k)})$ is a set of sensitivity parameters \citep{dh2008},
where $k < j, 2 \leq j \leq J $.

When $\bm \xi_s = \bm \xi_{s0} = \bm 0$, MAR holds. If $\bm \xi_s$ is
fixed at $\bm \xi_s \neq \bm \xi_{s0}$, the missingness mechanism is
MNAR.

For fully Bayesian inference, We can put independent priors on $(\bm
\xi_s, \bm \xi_m)$ as :
\begin{displaymath}
  p(\bm \xi_s, \bm \xi_m) = p(\bm \xi_s) p(\bm \xi_m),
\end{displaymath}
where $\bm \xi_m = \big(\bm \gamma_j, \bm \beta_{y, j-1}^{(\geq j)}, \bm
\alpha_j^{(\geq j)}, \bm \pi \big)$.

If we assume MAR with no uncertainty, the prior of $\bm \xi_s$ is
$\pr(\bm \xi_s = \bm 0) \equiv 1$. Sensitivity analysis can be
executed by putting point mass priors on $\bm \xi_s$ to examine the
effect of priors on the posterior inference about quantile regression
coefficients $\bm \gamma_{ij}^{\tau}$. For example, if MAR is assumed
with uncertainty, priors can be assigned as $\textrm{E}(\bm \xi_s) =
\bm \xi_{s0} = \bm 0$ with $\textrm{Var}(\bm \xi_s) \neq \bm 0$. If we
assume MNAR with no uncertainty, we can put priors satisfying
$\textrm{E}(\bm \xi_s) = \Delta_{\xi}$, where $\Delta_{\xi} \neq \bm
0$ and $\textrm{Var}(\bm \xi_s) = \bm 0$. If MNAR is assumed with
uncertainty, then priors could be $\textrm{E}(\bm \xi_s) =
\Delta_{\xi}$, where $\Delta_{\xi} \neq \bm 0 $ and $\textrm{Var}(\bm
\xi_s) \neq \bm 0$.

\subsection{Computation}
\label{sec:computation}

In section \ref{sec:deltacal}, we provide details on calculating
$\Delta_{ij}$ in (\ref{eq:model}) for $j = 1, \ldots, J$. Then we
show how to obtain maximum likelihood estimates using an adaptive
gradient descent algorithm in section \ref{sec:mle}. Finally, we
present a MCMC sampling algorithm for Bayesian inference in section
\ref{sec:bayesian}.

\subsubsection{Calculation of $\Delta$ }
\label{sec:deltacal}
From equation (\ref{eq:deltaeqn1}) and (\ref{eq:deltaeqn2}),
$\Delta_{ij}$ depends on subject-specific covariates $\bm x_i$, thus
$\Delta_{ij}$ needs to be calculated for each subject. We now
illustrate how to calculate $\Delta_{ij}$ given all the other
parameters $\bm \xi = (\bm \xi_m, \xi_s)$.

\begin{itemize}
\item \textbf{$\Delta_{i1}: $} Expand equation (\ref{eq:deltaeqn1}):
  \begin{align*}
    \tau = \sum_{k = 1}^J \pi_k \Phi \left( \frac{\bm x_{i1}^T \bm
        \gamma_1 - \Delta_{i1} - \bm x_{i1}^T\bm \beta_1^{(k)}}{\exp
        \big( \bm x_{i1}^T \bm \alpha_1^{(k)} \big)} \right),
  \end{align*}
  where $\Phi$ is the standard normal CDF. Because the above equation
  is continuous and monotone in $\Delta_{i1}$, it can be solved by a
  standard numerical root-finding method (e.g. bisection method) with
  minimal difficulty.

\item \textbf{$\Delta_{ij}, 2\leq j \leq J: $}

  First we introduce a lemma:
  \begin{lem}\label{sec:lemma}
    An integral of a normal CDF with mean $b$ and standard deviation
    $a$ over another normal distribution with mean $\mu$ and standard
    deviation $\sigma$ can be simplified to a closed form in terms of
    normal CDF:
    \begin{equation}
      \label{eq:lem}
      \int \Phi \left( \frac{x-b}{a} \right) d\Phi(x; \mu, \sigma)  =
      \begin{cases}
        1- \Phi \left( \frac{b-\mu}{\sigma} \big / \sqrt{\frac{a^2}{\sigma^2}+1} \right) & a > 0, \\
        \Phi \left( \frac{b-\mu}{\sigma} \big /
          \sqrt{\frac{a^2}{\sigma^2}+1} \right) & a < 0,
      \end{cases}
    \end{equation}
    where $\Phi(x; \mu, \sigma)$ stands for a CDF of normal
    distribution with mean $\mu$ and standard deviation $\sigma$.
  \end{lem}
  Proof of \ref{sec:lemma} is in Appendix \ref{sec:proof}.

  To solve equation (\ref{eq:deltaeqn2}), we propose a recursive
  approach. For the first multiple integral in equation
  (\ref{eq:deltaeqn2}), apply lemma \ref{sec:lemma} once to obtain:
  \begin{align*}
    \prob (Y_j \leq \bm x^T \bm \gamma_j | S = 1) & = \int\dots\int
    \prob (Y_j \leq \bm x^T\bm \gamma_j | S=1, \bm x, Y_{j-1}, \ldots, Y_1)\\
    & \quad  dF(Y_{j-1}|S=1, Y_{j-2}, \ldots, Y_1) \cdots dF(Y_2|S=1, Y_1) d F (Y_1 | S = 1), \\
    & = \int\dots\int
    \Phi \left( \frac{\bm x^T \bm \gamma_j - \mu_{j|1, \ldots, j-1}(y_{j-1})}{\sigma_{j|1, \ldots, j-1}} \right) \\
    & \quad dF(Y_{j-1}|S=1, Y_{j-2}, \ldots, Y_1) \cdots dF(Y_2|S=1, Y_1) d F (Y_1 | S = 1), \\
    & = \int\dots\int \Phi \left( \frac{y_{j-2} - b^{*}}{a^{*}}
    \right) dF(Y_{j-2}|S=1, Y_{j-3}, \ldots, Y_1) \cdots d F (Y_1 | S
    = 1).
  \end{align*}

  Then, by recursively applying lemma \ref{sec:lemma} $(j-1)$ times,
  each multiple integral in equation (\ref{eq:deltaeqn2}) can be
  simplified to single normal CDF. Thus it can be solved using
  standard numerical root-find method for $\Delta_{ij}$ as for $j =
  1$.

\end{itemize}

\subsubsection{Maximum Likelihood Estimation}
\label{sec:mle}

The observed data likelihood for an individual $\bm y_i$ with
follow-up time $S = k$ is
\begin{align} \label{eq:ll} L_i(\bm \xi| \bm y_i, S_{i} = k) & =
  \pi_k\pr_k (y_k | y_1, \ldots, y_{k-1})
  \pr_k (y_{k-1}|y_1, \ldots, y_{k-2}) \cdots \pr_{k} (y_1), \\
  & = \pi_k \pr_{\geq k} (y_k | y_1, \ldots, y_{k-1}) \pr_{\geq k-1}
  (y_{k-1}|y_1, \ldots, y_{k-2}) \cdots \pr_{k} (y_1), \nonumber
\end{align}
where $\bm y_i = (y_1, \ldots, y_k)$.

We use an adaptive gradient descent algorithm to compute the maximum
likelihood estimates \citep{ried1993}. Denote $J(\bm \xi) = - \log L =
- \log \sum_{i = 1}^n L_i$.  Then maximizing the likelihood is
equivalent to minimize the target function $J(\bm \xi)$. Under an MAR
assumption, we fix $\bm \xi_s = \bm 0$, while under MNAR assumption,
$\bm \xi_s $ can be chosen as desired.

During each step of the algorithm, $\Delta_{ij}$ has to be calculated
for each subject and at each time, as well as partial derivatives for
each parameter.

As an example of the speed of the algorithm, for 500 bivariate
outcomes and 4 covariates, it takes about 11 seconds for 70 iterations
to get convergence using R version 2.15.3 (2013-03-01) \citep{R} and
platform: x86\_64-apple-darwin9.8.0/x86\_64 (64-bit). Main parts of
the algorithm are coded in Fortran such as calculation of numerical
derivatives and log-likelihood to quicken computation.

Further details about the maximization algorithm are presented in
Appendix \ref{sec:agda}.

\subsubsection{Bayesian Framework}
\label{sec:bayesian}

For a Bayesian inference, we specify priors on the parameters $\bm
\xi$ and use a block Gibbs sampling method to draw samples from the
posterior distribution. Denote all the parameters (including
sensitivity parameters) to sample as :
\begin{displaymath}
  \bm \xi_m = \left( \bm \gamma_1, \bm \gamma_2, \ldots, \gamma_J,
    \bm \beta_{y,j-1}^{(\geq j)}, \bm \alpha_j^{(\geq j)}
    \text{ for } j = 1, \ldots, J \right),
  \bm \xi_s = \left( \bm h_j^{(k)}, \bm \eta_{j-1}^{(k)}, \bm \delta_j^{(k)}
    \text{ for } k = 1, \ldots, j; 2 \leq j \leq J \right).
\end{displaymath}
Comma separated parameters are marked to sample as a block.  Updates
of $\bm \xi_m$ require Metropolis-Hasting algorithm, while $\bm \xi_s$
samples are drawn directly from priors as desired for missingness
mechanism assumptions.

As mentioned in section \ref{sec:sa}, MAR or MNAR assumptions are
implemented via specific priors. For example, if MAR is assumed with
no uncertainty, then $ \bm \xi _s= \bm 0$ with probability 1. Details
for updating parameters are:

\begin{itemize}
\item $\bm \gamma_{1} $: Use Metropolis-Hasting algorithm.
  \begin{enumerate}
  \item Draw ($\bm \gamma_{1}^c$) candidates from candidate
    distribution;
  \item Based on the new candidate parameter $\bm \xi^c$, calculate
    candidate $\Delta_{i1}^c$ for each subject $i$ as we described in
    section \ref{sec:deltacal}. If $S > 1$ for subject $i$, update
    candidate $\Delta_{ij}^c, j \geq 2$ as well since $\Delta_{ij}, j
    \geq 2$ depends on $\Delta_{i1}$. (For $S = 1$, we only need to
    update $\Delta_{i1}^c$);
  \item Plug in $\Delta_{i1}^c$ or ($\Delta_{i1}^c, \Delta_{ij}^c, j
    \geq 2$) in likelihood (\ref{eq:ll}) to get candidate likelihood;
  \item Compute Metropolis-Hasting ratio, and accept the candidate
    value or keep the previous value.
  \end{enumerate}
\item For the rest of the identifiable parameters, algorithms for
  updating the samples are all similar to $\bm \gamma_j$.
\item For sensitivity parameters, because we do not get any
  information from the data, we sample them from priors, which are
  specified from the missingness mechanism assumptions.
\end{itemize}

\section{Simulation Study}
\label{sec:simulation}
In this section, we compared the performance of our proposed model in
section \ref{sec:settings} with the \textit{rq} function in
\textit{quantreg} R package \citep{quantreg}. The \textit{rq} function
minimizes the loss (check) function $\sum_{i=1}^n \rho_{\tau} (y_i -
\bm x_i^T \bm \beta)$ in terms of $\bm \beta$, where the loss function
$\rho_{\tau} (u) = u(\tau - I(u < 0))$ and does not make any
distributional assumptions.

We considered two scenarios corresponding to MAR and MNAR
assumptions. For each scenario, we simulated 1000 data sets. For each
set there are 200 bivariate observations $\bm Y_i = (Y_{i1}, Y_{i2})$
for $i = 1, \ldots, 200$. $Y_{i1}$ were always observed, while some of
$Y_{i2}$ were missing. A single covariate $x$ was sampled from
Uniform(0,2). The two models for the full data response $\bm Y_i$
were:

\begin{enumerate}
\item \label{sim:mar} $ Y_{i1} |R = 1 \sim N ( 2 + x, 1 + 0.5x)$, $
  Y_{i2} | R = 1, y_{i1} \sim N(1 - x - 1/2y_{i1}, 1) $, $ Y_{i1}| R=
  0 \sim N(-2 - x, 1 + 0.5x) $, $ Y_{i2}| R= 0, y_{i1} \sim N(1 - x -
  1/2y_{i1}, 1)$;

\item \label{sim:mnar} $ ( Y_{i1}, Y_{i2}) |R = 1 \sim \textrm{N}
  \big( ( 1 + x, 1 - x), (\sigma_1 = 1, \rho = 0, \sigma_2 = 1)
  \big)$, $ Y_{i1} | R = 0 \sim \textrm{N}(-1-x, 1) $, $ Y_{i2} | R =
  1 \sim \textrm{N}(3-x, 1) $,
\end{enumerate}
where $\textrm{N}\big( (\mu_1, \mu_2), (\sigma_1, \rho, \sigma_2)
\big)$ stands for bivariate normal distribution with two marginal
normal distribution $\textrm{N}(\mu_1, \sigma_1)$, $\textrm{N}(\mu_2,
\sigma_2)$ and correlation $\rho$.

For all cases, $\prob (R = 1) = 0.5$.  When $R = 0$, $Y_{i2}$ is not
observed, so $\pr(Y_{i2}| R = 0, y_{i1})$ is not identifiable from
observed data. The missingness mechanism is MAR in case \ref{sim:mar}
and MNAR in case \ref{sim:mnar}.

Under MAR assumption, the sensitivity parameter $\bm \xi_s$ is fixed
at $(0,0,0,0,0)$ as discussed in section \ref{sec:sa}. For \textit{rq}
function from \textit{quantreg} R package, because only $Y_{i2}|R = 1$
is observed, the quantile regression for $Y_{i2}$ can only be fit from
the information of $Y_{i2}|R = 1$ vs $x$.

Under MNAR scenario, we fixed $\bm \xi_s$ at the true value $(2, 0, 0,
0, 0)$, assuming there was an intercept shift between distribution of
$Y_{i2}|Y_{i1}, R = 1$ and $Y_{i2}|Y_{i1}$, $R = 0$.

For each dataset, we fit quantile regression for quantiles $\tau =$
0.1, 0.3, 0.5, 0.7, 0.9.

Parameter estimates were evaluated by mean squared error (MSE),
\begin{equation*}
  \text{MSE} (\gamma_{ij}) = \frac{1}{1000} \sum_{k = 1}^{1000}
  \left( \hat{\gamma}_{ij}^{(k)}  - \gamma_{ij}\right)^2,
\end{equation*}
where $\gamma_{ij}$ is the true value for quantile regression
coefficient, $\hat{\gamma}_{ij}^{(k)}$ is the maximum likelihood
estimates in $k$-th simulated dataset ($(\gamma_{01}, \gamma_{11})$
for $Y_{i1}$, $(\gamma_{02}, \gamma_{12})$ for $Y_{i2}$).

Simulation results show estimates from our algorithm are closer to the
true value for all quantiles from 0.1 to 0.9. Table \ref{tab:simh2}
and \ref{tab:sim2} present the MSE for coefficients estimates of
quantile 0.1, 0.3, 0.5, 0.7, 0.9 under MAR and MNAR assumptions.  They
show that our proposed method has smaller MSE than \textit{rq}
function in all cases. When data are missing at random, our method
provides larger gains over \textit{rq} method, because
\textit{quantreg} does not consider the missingness mechanism. The
difference in MSE becomes larger for the upper quantiles because $Y_2
|R = 0$ tends to be larger than $Y_2 | R = 1$; therefore, the
\textit{rq} method using only the observed $Y_2$ yields larger bias
for upper quantiles.  For the same reason, under the MNAR assumption,
'quantreg' method led to much larger MSEs than our proposed method.

\begin{table}[ht]
  \renewcommand{\arraystretch}{1.3}
  \centering
  \caption{Simulation result: MSE for coefficients estimates of quantiles
    0.1, 0.3, 0.5, 0.7, 0.9 under MAR assumptions. $(\gamma_{01}, \gamma_{11})$
    are quantile regression coefficients for $Y_{i1}$, and $(\gamma_{02}, \gamma_{12})$
    are ones for $Y_{i2}$. MM stands for our proposed method, and RQ stands for the 'rq'
    function in R package 'quantreg'.}\label{tab:simh2}
  \vspace{10pt}
  \begin{tabular}{rrrrrrrrrrr}
    \toprule
    & \multicolumn{ 10}{c}{MAR} \\
    \cline{2-11}
    &  \multicolumn{2}{c}{0.1} &  \multicolumn{2}{c}{0.3} &  \multicolumn{2}{c}{0.5} &
    \multicolumn{2}{c}{0.7} &  \multicolumn{2}{c}{0.9} \\
    \cline{2-11}
    & MM & RQ    & MM & RQ    & MM & RQ    & MM & RQ    & MM & RQ \\
    \hline
    $\gamma_{01}$ & 0.09 & 0.15 & 0.12 & 0.19 & 0.11 & 1.08 & 0.16 & 0.19 & 0.10 & 0.15 \\
    $\gamma_{11}$ & 0.09 & 0.15 & 0.07 & 0.19 & 0.14 & 1.19 & 0.08 & 0.20 & 0.10 & 0.15 \\
    $\gamma_{02}$ & 0.08 & 0.27 & 0.07 & 0.59 & 0.06 & 1.08 & 0.12 & 1.75 & 0.24 & 2.92 \\
    $\gamma_{12}$ & 0.06 & 0.17 & 0.05 & 0.13 & 0.06 & 0.33 & 0.07 & 0.75 & 0.09 & 0.96 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \renewcommand{\arraystretch}{1.3}
  \centering
  \caption{Simulation result: MSE for coefficients estimates of quantiles
    0.1, 0.3, 0.5, 0.7, 0.9 under MNAR scenario. $(\gamma_{01}, \gamma_{11})$
    are quantile regression coefficients for $Y_{i1}$, and $(\gamma_{02}, \gamma_{12})$
    are ones for $Y_{i2}$. MM stands for our proposed method, and RQ stands for the 'rq'
    function in R package 'quantreg'.}
  \vspace{10pt}
  \begin{tabular}{rrrrrrrrrrr}
    \toprule
    & \multicolumn{ 10}{c}{MNAR} \\
    \cline{2-11}
    &  \multicolumn{2}{c}{0.1} &  \multicolumn{2}{c}{0.3} &  \multicolumn{2}{c}{0.5}
    &  \multicolumn{2}{c}{0.7} &  \multicolumn{2}{c}{0.9} \\
    \cline{2-11}
    & MM & RQ    & MM & RQ    & MM & RQ    & MM & RQ    & MM & RQ \\
    \hline
    $\gamma_{01}$ & 0.04 &0.09&0.04 &0.10 &0.03 &0.24 &0.04 &0.10 &0.04 &0.10 \\
    $\gamma_{11}$ & 0.03 &0.07&0.02 &0.08 &0.64 &0.74 &0.03 &0.08 &0.03 &0.07 \\
    $\gamma_{02}$ & 0.04 &0.30&0.05 &0.52 &0.07 &1.06 &0.05 &1.79 &0.05 &2.59 \\
    $\gamma_{12}$ & 0.03 &0.09&0.03 &0.05 &0.03 &0.05 &0.03 &0.05 &0.03 &0.09 \\
    \bottomrule
  \end{tabular}  \label{tab:sim2}
\end{table}

\section{Real Data Analysis}
\label{sec:real}
We apply our quantile regression approach to data from TOURS, a weight
management clinical trial \citep{perri2008extended}.  This trial was
designed to test whether a lifestyle modification program could
effectively help people to manage their weights in long term. After
finishing the six-month program, participants were randomly assigned
to three treatments groups: face-to-face counseling, telephone
counseling and control group. Their weights were recorded at baseline
($Y_0$), 6 months ($Y_1$) and 18 months ($Y_2$) after the trial. Here,
we are interested in how the distribution of weights at six months and
eighteenth months change with covariates. The regressors of interest
include AGE (50-75), RACE (black and white) and weights at baseline
($Y_0$). Weights at the six months ($Y_1$) were always observed and 13
out of 224 observations (6\%) were missing at 18 months ($Y_2$). All
weights were scaled by 1/100 for computation stability.

We fitted regression models for bivariate responses $\bm Y_i =
(Y_{i1}, Y_{i2})$ for quantiles (5\%, 30\%, 50\%, 70\%, 95\%).  We fit
1,000 bootstraps to obtain the 95\% confidence intervals.

Estimates are presented in Table \ref{tab:w2}. For weights of
participants at six months, weights of white people are generally
lower than ones of black people, because for all quantiles, the
coefficients of race are negative. For the extreme quantiles (5\% and
95\%), the difference is small and not significant. However, when
comparing the weights for quantiles 30\%, 50\% and 70\%, white people
tend to weight significantly less than black people. The differences
are 8kg, 7kg, and 8kg separately. Meanwhile, weights of participants
show obvious heterogeneity by age. Participants tend to have less
weight when older. The trend is for all quantiles, but the effect is
again small and not significant.

\begin{table}[ht]
  \renewcommand{\arraystretch}{1.3}
  \begin{center}
    \caption{Estimated marginal quantile regression coefficients with
      95\% confidence interval for weight of participants at 6 and 18
      months. Weight measurement is scaled by 1/100.}\label{tab:w2}
    \vspace{10pt}
    \begin{tabular}{rrrr}
      \toprule
      & Intercept            & Age(Centered)          & White                  \\
      \hline
      Weight at 6 months                                                         \\
      5\%  & 0.65  (0.34, 0.73 ) & -0.03 ( -0.10, 0.06 ) & -0.02 ( -0.48, 0.18 ) \\
      30\% & 0.86  (0.79, 0.90 ) & -0.02 ( -0.03, 0.00 ) & -0.08 ( -0.12, -0.01) \\
      50\% & 0.92  (0.87, 0.97 ) & -0.01 ( -0.03, 0.01 ) & -0.07 ( -0.12, -0.02) \\
      70\% & 1.00  (0.94, 1.05 ) & -0.01 ( -0.04, 0.02 ) & -0.08 ( -0.13, -0.01) \\
      95\% & 1.12  (1.08, 1.26 ) & -0.01 ( -0.06, 0.05 ) & -0.03 ( -0.14, 0.12 ) \\
      Weight at 18 months                                                        \\
      5\%  & 0.10  (-0.01, 0.64 ) & -0.00 ( -0.05, 0.01 ) & -0.02 ( -0.19, 0.22 ) \\
      30\% & 0.84  (0.77, 0.88 ) & -0.02 ( -0.04, -0.00) & -0.06 ( -0.11, 0.01 ) \\
      50\% & 0.92  (0.87, 0.98 ) & -0.02 ( -0.04, 0.01 ) & -0.06 ( -0.11, -0.00) \\
      70\% & 1.02  (0.94, 1.06 ) & -0.01 ( -0.04, 0.02 ) & -0.06 ( -0.12, 0.01 ) \\
      95\% & 1.15  (1.09, 1.25 ) & -0.01 ( -0.05, 0.04 ) & -0.02 ( -0.13, 0.05 ) \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

For weights at 18 months after baseline, we have similar
conclusions. White people still have less weight than black people for
all quantiles, but the magnitude is smaller than that at 6th month.
The differences are 6kg, 6kg and 6kg instead of 8kg, 7kg and 8kg for
quantiles 30\%, 50\% and 70\%. It still shows a decreasing trend of
weights over ages.  However, none of them shows significance for the
weight difference.

\section{Discussion}
\label{sec:discussion}

In this paper, we have developed a marginal quantile regression model
for data with monotone missingness. We use a pattern mixture model to
jointly model the full data response and missingness. Here marginal
quantile regression coefficients are of interest instead of
coefficients conditional on random effects as in \citet{yuan2010}. In
addition, our approach allows non-parallel quantile lines over
different quantiles via the mixture distribution and heterogeneity of
variance.

Our method allows the missingness to be MNAR.  We illustrated how to
put informative priors for Bayesian inference and how to find
sensitivity parameters to allow different missing data mechanisms.
The recursive integration algorithm simplifies computation and can be
easily implemented even in high dimension.  Simulation study
demonstrates that our approach has smaller MSE than the traditional
frequentist method and it allows for MAR and MNAR missingness.

Our model assumes a multivariate normal distribution for each
component in the pattern mixture model, which might be too
restrictive. It is possible to replace that with a semi-parametric
model, for example, the Dirichlet process mixture or \polya{} tree.
It would also be interesting to allow mixture probabilities depend on
covariates. Our future work will also include development of a
goodness of fit test to the model fit.

\section{Acknowledgments}


\bibliographystyle{plainnat}
% \bibliographystyle{abbrev}
\bibliography{qr-missing-reference}

\appendix
\section{Identifiability}
\label{sec:iden}
First suppose $y$ is univariate and there are two patterns $R = 1$ and
$R = 0$.

Before going forward to quantile regression, first we consider
identifiability problem in mean regression.

Consider a pattern mixture model:
\begin{align}
  Y | R = 1 & \sim N(\Delta + \mu_1, \sigma_1), \label{eq:app1} \\
  Y | R = 0 & \sim N(\Delta + \mu_0, \sigma_0), \nonumber\\
  \prob (R = 1) & = \pi, \nonumber\\
  E (y ) & = \theta. \nonumber
\end{align}
Thus by iterated expectation, we have
\begin{align*}
  \theta = \Delta + \mu_1\pi + \mu_0(1-\pi), \\
  \Delta = \theta - \pi \mu_1 - (1 - \pi)\mu_0.
\end{align*}
We can see $\Delta$ is determined by $\theta, \mu_1, \mu_0$. Plugging
in (\ref{eq:app1}), we have
\begin{align*}
  Y| R = 1 & \sim N(\theta + (1 - \pi)\mu_1 - (1 - \pi)\mu_0, \sigma_1), \\
  Y| R = 0 & \sim N(\theta - \pi \mu_1 + \pi \mu_0, \sigma_0).
\end{align*}
Denote $\xi_1 = (\theta, \mu_1, \mu_0)$, and if $\xi_2 = (\theta,
\mu_1+ c, \mu_0+c)$, both groups of parameters lead to the same
distribution of $\pr(y, R) = \pr(y|R)\pr(R)$. Therefore, $\xi$ is not
identifiable.  If we put constraints on $\mu_1$ and $\mu_0$, for
example $\mu_0 = 0$, then
\begin{align*}
  Y | R = 1 & \sim N(\theta + \mu_1, \sigma_1), \\
  Y | R = 0 & \sim N(\theta, \sigma_0).
\end{align*}
Thus $\bm \xi = (\theta, \mu_1)$ is identifiable. If $\xi_2 \neq
\xi_1$, then $\pr_2(y, R) \neq \pr_1(y, R)$.

Secondly, we consider quantile regression for a pattern mixture model:
\begin{align*}
  Y | R = 1 & \sim N(\Delta + \mu_1, \sigma_1),\\
  Y | R = 0 & \sim N(\Delta + \mu_0, \sigma_0),\\
  \prob (R = 1) & = \pi,\\
  \pr (Y \leq \theta ) & = \tau,
\end{align*}
where $\theta$ is the quantile estimate of interest. We again show
$\bm \xi = (\theta, \mu_1, \mu_0) $ is not identifiable.

Again by iterated expectations, we have
\begin{align*}
  \tau = \pi \Phi \left( \frac{\theta - \Delta - \mu_1}{\sigma_1}
  \right) + (1 - \pi) \Phi \left( \frac{\theta - \Delta -
      \mu_0}{\sigma_0} \right).
\end{align*}
Thus $\Delta$ is again determined by the other parameters:
\begin{align*}
  \Delta = h(\theta, \mu_1, \mu_0, \sigma_1, \sigma_0, \pi, \tau).
\end{align*}
To show $\bm \xi = (\theta, \mu_1, \mu_0, \sigma_1, \sigma_0)$ is not
identifiable, we need to find $\bm \xi^{'} \neq \bm \xi$, such that
$\pr(y|R) = \pr^{'}(y|R)$. If the last equation holds, then we must
have $\sigma_1^{'} = \sigma_1, \sigma_0^{'} = \sigma_0$, thus we still
need to find $\theta^{'}, \mu_1^{'}, \mu_0^{'}$ such that
\begin{align*}
  h(\bm \xi) + \mu_1 & = h(\bm \xi^{'}) + \mu_1^{'},\\
  h(\bm \xi) + \mu_0 & = h(\bm \xi^{'}) + \mu_0^{'}.
\end{align*}
By substracting previous equations, we have $\mu_1^{'}- \mu_0^{'} =
\mu_1- \mu_0$. Denote $\mu_1^{'} = \mu_1 + \delta$ and $\mu_0^{'} =
\mu_0 + \delta$, and let $\theta^{'} = \theta$ such that
\begin{align*}
  \Delta^{'} = h(\theta^{'}, \mu_1, \mu_0, \sigma_1, \sigma_0, \delta)
  = h(\bm \xi) - \delta = \Delta - \delta.
\end{align*}
Then the new parameter $\bm \xi^{'}$ yields the same distribution as
$\bm \xi$. Therefore $\bm \xi$ is not identifiable.

If we use a constraint, for example $\mu_1 = -\mu_0$, then
$\pr(y|R;\bm \xi) = \pr(y|R; \bm \xi^{'})$ yields $\bm \xi = \bm
\xi^{'}$.

Now consider the situation with covariates. Suppose the model is
\begin{align*}
  Y | R = 1, x & \sim N(\Delta + \mu_1 + \beta_{x1} x, \sigma_1),\\
  Y | R = 0, x & \sim N(\Delta - \mu_1 + \beta_{x0} x, \sigma_0),\\
  \prob (R = 1) & = \pi,\\
  \pr (Y \leq \gamma_0 + \gamma_1 x ) & = \tau.
\end{align*}
$\Delta$ can still be determined by
\begin{align*}
  \Delta = h(x, \gamma_0, \gamma_1, \mu_1, \beta_{x1}, \beta_{x0},
  \sigma_1, \sigma_0, \pi, \tau).
\end{align*}
We want to show the parameter $\bm \xi = (\gamma_0, \gamma_1, \mu_1,
\beta_{x1}, \beta_{x0}, \sigma_1, \sigma_0, \pi )$ is not identifiable
by finding $\bm \xi^{'} \neq \bm \xi$, but $\pr (y | R; \bm \xi) = \pr
(y | R; \bm \xi^{'})$. If the last equation holds, we have
$\sigma_1^{'} = \sigma_1, \sigma_0^{'} = \sigma_0$, and to equate the
two means, we have
\begin{align*}
  \Delta + \mu_1 + \beta_{x1} x & = \Delta^{'} + \mu_1^{'} + \beta_{x1}^{'}x,\\
  \Delta - \mu_1 + \beta_{x0} x & = \Delta^{'} - \mu_1^{'} +
  \beta_{x0}^{'}x.
\end{align*}
By substracting the two equations, we have
\begin{align*}
  2\mu_1 + (\beta_{x1} - \beta_{x0}) x = 2\mu_1^{'} + (\beta_{x1}^{'}
  - \beta_{x0}^{'}) x,
\end{align*}
which holds for all $x$. Thus $\mu_1 = \mu_1^{'}$ and $(\beta_{x1} -
\beta_{x0}) = (\beta_{x1}^{'} - \beta_{x0}^{'})$. Then let
\begin{align*}
  \beta_{x1}^{'} =  \beta_{x1} + \delta, \\
  \beta_{x0}^{'} = \beta_{x0} + \delta,
\end{align*}
and keep all the other parameters in $\bm \xi^{'}$ the same. We can
still have the same distribution of $y|R; \bm \xi$ but with different
$\bm \xi$. Therefore, $\bm \xi$ is not identifiable One solution is to
restrict $\beta_{x1} = - \beta_{x0}$ or $\beta_{x1} = 0$.

Now consider the bivariate $(y_1, y_2)$ case, and we focus on the
identifiability issue especially $y_2|y_1$. Suppose the model is
\begin{align*}
  Y_2 | y_1, x, R = 1 \sim N(\Delta + \mu_1 + x\beta_{x1} + \beta_{11}y_1, \sigma_1), \\
  Y_2 | y_1, x, R = 0 \sim N(\Delta - \mu_1 - x\beta_{x1} +
  \beta_{10}y_1, \sigma_0).
\end{align*}
Here $R$ stands for two different patterns, and missingness is not
considered.

Regarding the identifiability of $\beta_{11}$ and $\beta_{10}$,
assume there exists $\beta_{11}^{'}$ and $\beta_{10}^{'}$, such that
\begin{align*}
  \Delta + \mu_1 + x\beta_x + \beta_{11}y_1 = \Delta^{'} + \mu_1^{'} + x\beta_{x}^{'} + \beta_{11}^{'}y_1, \\
  \Delta - \mu_1 - x\beta_x + \beta_{10}y_1 = \Delta^{'} - \mu_1^{'} -
  x\beta_{x}^{'} + \beta_{10}^{'}y_1.
\end{align*}
By substracting two equations, we have $\mu_1 = \mu_1^{'}$ and
$\beta_x = \beta_x^{'}$. Since $\Delta$ is determined by integrating
out $y_1$, such that matching the two sides of the above equation for
coefficient of $y_1$, we must have $\beta_{11} = \beta_{11}^{'}$ and
$\beta_{10} = \beta_{10}^{'}$, therefore, $\bm \xi$ is identifiable.

For identifiability issue with the heterogeneous model described in
section \ref{sec:settings}, it is easy to show there is no trouble
with the heterogeneity parameters $\alpha$, analogous to the linear
model case. For the other parameters, it can be found similar to the
above development.

\section{Proof of Lemma \ref{sec:lemma}}
\label{sec:proof}
\begin{itemize}
\item Denote
  \begin{displaymath}
    I(a,b) = \int \Phi \left( \frac{x-b}{a} \right)\phi(x) dx,
  \end{displaymath}
  where $\Phi$ is the standard normal cdf and $\phi$ is the standard
  normal pdf and $a > 0$.
  \begin{align*}
    \frac{\partial I(a,b)}{\partial b} & = - \frac{1}{a} \int \phi \left( \frac{x-b}{a} \right) \phi(x) dx \\
    & = - \frac{1}{\sqrt{2 \pi} \sqrt{a^2+1}} \exp \left( - \frac{b^2}{2(a^2+1)} \right)\\
    & = -\frac{1}{\sqrt{a^2+1}} \phi \left( \frac{b}{\sqrt{a^2+1}}
    \right).
  \end{align*}
  Since $I(a, \infty) = 0$,
  \begin{align}
    I(a,b) &= - \frac{1}{\sqrt{a^2+1}} \int_b^{\infty} \phi \left( \frac{s}{\sqrt{a^2+1}} \right) ds \nonumber \\
    &= \int_{b/\sqrt{a^2+1}}^{\infty} \phi(t) dt \nonumber\\
    \label{eq:int}
    & = 1- \Phi(b/\sqrt{a^2+1}).
  \end{align}
  For $a < 0$,
  \begin{align*}
    \frac{\partial I(a,b)}{\partial b} & = - \frac{1}{a} \int \phi \left( \frac{x-b}{a} \right) \phi(x) dx \\
    & = - \frac{sgn(a)}{\sqrt{2 \pi} \sqrt{a^2+1}} \exp \left( - \frac{b^2}{2(a^2+1)} \right)\\
    & = -\frac{sgn(a)}{\sqrt{a^2+1}} \phi \left(
      \frac{b}{\sqrt{a^2+1}} \right).
  \end{align*}
  Since $I(a, -\infty) = 0$:
  \begin{align}
    I(a,b) &= \int^{b/\sqrt{a^2+1}}_{-\infty} \phi(t) dt \nonumber\\
    \label{eq:intneg}
    & = \Phi(b/\sqrt{a^2+1}).
  \end{align}
\item For integrating over a normal distribution with mean $\mu$ and
  standard deviation $\sigma$:
  \begin{align*}
    \int \Phi(x)d\Phi(x; \mu, \sigma) & = \int \Phi(x) \frac{1}{\sigma} \phi \left( \frac{x-\mu}{\sigma} \right) dx \\
    & = \int \Phi(\sigma t + \mu)\phi(t) dt \\
    & = 1 - \Phi(-\mu/\sigma/\sqrt{1/\sigma^2+1}).
  \end{align*}
  The last equation holds by (\ref{eq:int})
\item For integrating a $\textrm{N}(b, a)$ CDF over another normal
  distribution ($\textrm{N}(\mu, \sigma$)):
  \begin{align}
    \int \Phi \left( \frac{x-b}{a} \right) d\Phi(x; \mu, \sigma) & = \int \Phi \left( \frac{x-b}{a} \right) \frac{1}{\sigma} \phi \left( \frac{x-\mu}{\sigma} \right) dx \nonumber\\
    &= \int \Phi \left( \frac{\sigma y + \mu - b}{a}  \right) \phi(y) dy \nonumber \\
    \label{eq:intg1}
    & = 1- \Phi \left( \frac{b-\mu}{\sigma} /
      \sqrt{\frac{a^2}{\sigma^2}+1} \right).
  \end{align}
  If $a < 0$,
  \begin{equation}
    \label{eq:intg2}
    \int \Phi \left( \frac{x-b}{a} \right) d\Phi(x; \mu, \sigma) = \Phi \left( \frac{b-\mu}{\sigma} / \sqrt{\frac{a^2}{\sigma^2}+1} \right).
  \end{equation}

\end{itemize}

\section{Maximum Likelihood Estimation Using Adaptive Gradient Descent
  Algorithm}
\label{sec:agda}

See section \ref{sec:mle} for notation.

The likelihood can be maximized via the following algorithm:
\begin{enumerate}
\item initialize $\bm \xi$
\item \label{item:der} calculate $\partial J(\bm \xi) / \partial
  \xi_j$ for all $j$,
\item update $\bm \xi$ by adaptive gradient descent algorithm
  described in \citep{ried1993} for all $j$
\item evaluate new $J(\bm \xi)$
\item if the amount of descent of $J ( \bm \xi)$ is greater than a
  certain cutoff, then go back to step \ref{item:der} and
  repeat. Otherwise, algorithm converges.
\end{enumerate}

We can use numerical approximation to calculate $\partial J(\bm
\xi)/\partial \xi_j$ in step \ref{item:der}. For example, for $j = 1$,
\begin{displaymath}
  \frac{\partial J(\bm \xi)}{\partial \xi_1} \approx \frac{J(\xi_1 + \epsilon, \xi_2, \ldots) - J(\xi_1 - \epsilon, \xi_2, \ldots)}{2\epsilon}.
\end{displaymath}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
